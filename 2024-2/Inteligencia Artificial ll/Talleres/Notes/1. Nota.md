# 1. Nota

Keras ofrece una amplia gama de metricas, optimizadore y funciones de perdida, que son clave para configurar y entrenar redes neuronales, tanto para problemas de clasificacion como de regresion.

## Metricas en Keras para clasifcacion y regresion

### Clasificacion

* **Accuracy** Mide la proporcion de predicciones correctas

    * **Ventajas** Es facil de interpretar.
    * **Desventajas** No es adecuada para conjuntos de datos desvalanceados.
    
* **Precision** Mide la proporcion de verdaderos positivos sobre el total de predicciones positivas.
    
    * **Ventajas** Util cuando las falsa positivas son costosas.
    * **Desventajas** Ignora los falsos positivos.
    
* **Recall (Sencibilidad)** Mide la proporcion de verdaderos positivos sobre el total de positivos reales.
    
    * **Ventajas** Importante cuando es crucial identificar todos los casos positivos.
    * **Desventajas** No considera flasos positivos.
    
* **F1-Score** Promedio armonico de la precision y el recall.

    * **Ventajas** Equilibra precision y recall
    * **Desventajas** Es menos interpretable que la accuracy.
    
* **AUC (Area Under the Curve)** Mide el area bajo la curva ROC, que compara la taasa de verdaderos positivos contra falsos positivos.

    * **Ventajas** Mide la capacidad del modelo para diferenciar entre clases.
    * **Desventajas** Puede ser dificil de interpretar para clases desbalanceadas.
    
### Regresión

* **Mean Absolute Error (MAE)** Promedio de las diferencias absolutas entre las predicciones y los valores reales.

    * **Ventajas** Facil de interpretar
    * **Desventajas** Penaliza errores mas pequeños por igual que los grandes
    
* **Mean Squared Error (MSE)** Promedio de los cuadrados de los errores.

    * **Ventajas** Penaliza mas los errores grandes.
    * **Desventajas** Sensible a outliers
    
* **Root Mean Squared Error (RMSE)** Raiz cuadrada del MSE.

    * **Ventajas** Mas interpretable en terminos de unidades del problema.
    * **Desventajas** Aun sensible a outliers
    
* **R-squared**  Mide la proporcion de varianza explicada por el modelo.
   
    * **Ventajas**  Es una metrica estandar apra evaluar modelos de regrecion.
    * **Desventajas** No mide que tan bien predice el modelo en nuevos datos.
    

## Optimizadores en Keras

1. **SGD (Stochastic Gradient Descent)**

    * **Ventajas** Simple, eficiente
    * **Desvenajas** Puede quedarse atrapado en minimos locales.
  
2. **RMSprop** 

    * **Ventajas** Adaptable a cambios rapidos en la funcion de perdida.
    * **Desventaja** No es ideal para problemas con grandes fluctuaciones.
 
3. **Adam (Adaptative Moment Estimantion)**

    * **Ventajas** Combina las ventajas de los optimizadores RMSprop y Momentun y es adecuado apra redes robustas.
    * **Desventajas** Puede sobreajustar en algunos casos
    
 4. **Adagrad**
 
    * **Ventajas** Adecuado para datos dispersos y optimiza el uso de un learning rate por parametro.
    * **Desventajas** El learning rate tiende a volverse muy pequeño con el tiempo.
    
5. **Adadelta**

    * **Ventajas** No requiere ajuste manual del learning rate.
    * **Desventajas** A veces tiene una convergencia lenta.
    
## Funciones de perdidad en Keras

### Clasificacion

* **Categorical Crossentropy** Utilizado para clasificacion multiclase.

    * **Ventajas** Adecuado para problemas de multiples clases.
    * **Desventajas** Asume que las clases son mutuamente escluyentes.
    
* **Binary Crossentropy** Para problemas de clasificacion binaria

    * **Ventajas** Ajuste pra problemas de 2 clases.
    * **Desventaja** No funciona para mas de 2 clases.
    
* **Sparce Categorical Crossentropy** Similar a cetegorical crossentropy pero con etiquetas enteras.

    * **Ventajas** Mas eficiente en terminos de memoria.
    
### Regresión

* **Mean Squared Error (MSE)** Calcula la media del cuadrado de los errores.

    * **Ventajas** Comun y facil de interpretar.
    * **Desventajas** Penaliza mas los errores grandes.
    
* **Mean Absolute Error (MAE)** Promedi el error absoluto

    * **Ventajas** No es tan sensible a otuliers-
    * **Desventajas** Puede tener una convergencia mas lenta.
   
* **Huber Loss** Combina MSE y MAE penalizando menos los outliers.
    
    * **Ventajas** Robusta a outliers.
    * **Desventajas** Mas compleja de ajustar.
    

### Parametros DNN

#### Learning Rate

* El **learning rate** controla que tan grande son los pasos que da el modelo en la direccion de la minimizacion de la funcion de perdida.

    * **Ventajas de un learning rate bajo** Convergencia estable.
    * **Desventajas de un learning rate bajo** Convergencia lenta.
    * **Ventajas de un learning rate alto** Convergencia rapida.
    * **Desventajas de un learning rate alto** Riesgo de saltarse el minimo optimo.
    
#### Epocas (Epochs)

* Los **Epochs** son el numero de veces que el modelo pasa por todo el conjunto de datos de entrenamiento.

    * **Ventajas de mas Epochs** Mejora el ajuste del modelo.
    * **Desventajas de mas Epochs** Riesgo de sobreajuste (Overfiting).
    
### Funciones DNN

#### Sequential

* Es un conector de un modelo en Keras

    * **Uso** Se utiliza para aplicar capas de una manera secuencia (una tras otra).
    
#### add()

* Añade capas a un modelo secuencial

    * **Uso** Para apliar capas como  `Dense` , `Conv2D`
    
#### compile()

* Configura el modelo con un optimizador, una funcion de perdidad y metricas.

    * **Uso** Se utiliza para preparar el modelo para el entrenamiento.
    * **Ventajas** Define como se entrenara el modelo.
    * **Parametros Importantes** `optimizer`, `loss`, `metrics`.
    
#### summary()

* Muestra un resumen del modelo

    * **Uso** Para visualizar la arquitectura del modelo, numero de parametros y capas.
    
#### fit()

*  Entre el modelo con datos de entrenamiento.

    * **Uso** Ajusta los pesos del modelo basandose en los datos de entrenamiento.
    * **Ventajas** Permite el entrenamiento por lotes y el uso de validacion.
    * **Desventajas** Puede ser costoso computacionalmente para grandes datos.
    
#### evaluate()

* Evalua el rendimiento del modelo en datos de prueba

    * **Uso** Para obtener las metricas de evaluacion en un conjunto de datos no entrenados.
    

### Funciones de activacion

#### Sigmoid

* Comvierte cualquier valor de entrada en un numero entre 0 y 1. 

    * **Ventajas** Buena para problemas de clasificacion binaria, ya que el valor de salida es una probabilidad.
    * **Desventajas** Problemas de saturacion (Cuando los gradientes se acercan a 0) y vanishing gradients. No funciona bien en capas ocultas profundas.
    * **Uso** Salida de clasificacion binaria.
    
#### ReLU (Rectified Linear Unit)

 * Devuelve el valor de entrada si es positivo, y 0 si es negativo.
 
    * **Ventajas** Simple, eficiente y reduce el problema de vanishing gradientes, en redes profundas.
    * **Desventajas** Puede acusar problemas de <span style="color:#84E4DD">neurona muerta</span> (Cuando las neuronas dejen de activarse si la entrada es negativa)
    * **Uso** Capas ocultas en redes profundas.
    
#### Leaky ReLU

* Igual que ReLU, pero permite un pequeño gradiente para valores negativos, evitando la muerte de neuronas

    * **Ventajas** Evita el problema de <span style="color:#84E4DD">neurona muerta</span>.
    * **Desventajas** Añade un hiperametro (el slope para valores negativos)
    * **Uso** Capas ocultas, mejora sobre ReLU
    
#### Tanh

* Escala los valores de salida entre -1 y 1 

    * **Ventajas** Centra las salidas alrededor de 0, lo que puede ayudar en la convergencia.
    * **Desventajas** Problema de vanishing grandientes similar a Sigmoid
    * **Uso** Capas ocultas
    
#### Softmax

* Covierte el vector de salida en probabilidades. La formula es:

    * **Ventajas** Ideal para clasificacion multiclase
    * **Desventajas** No se recomienda para problemas de regresion,
    * **Uso** Salida en problemas de clasificacion multiclase.
    


